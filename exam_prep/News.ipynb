{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d70a7205-97fb-47b5-a39a-0c33649de6be",
   "metadata": {
    "editable": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-405d0428aa49a083",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# News"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a386cd-9613-49bb-b02b-3262c7364d4a",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a9e03819b98857c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this assignment, you will use the `news.csv` dataset to train a neural network that predicts a category of an article (science, politics or technology) by a given title.\n",
    "    \n",
    "***Notes:***\n",
    "\n",
    "- Some parts of the code are already provided. **Do not modify the existing code.**\n",
    "- Write your solution only in the sections marked with `### YOUR SOLUTION`.\n",
    "- You can verify automatically graded tasks using the cell labeled `### TEST` after each function.\n",
    "\n",
    "***IMPORTANT NOTE:***\n",
    "- Name your Jupyter Notebook as `news_{index}.ipynb`.\n",
    "- For example, if your index is 123456, you should name your notebook as `news_12346.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "384d477e-8b45-4508-82fc-b8d4fed214a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdaa0367-3a68-420d-9b85-fd376cae01f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_data_frame(df):\n",
    "    df_sorted = df.sort_index(axis=1).sort_values(by=list(df.columns))\n",
    "    return hashlib.sha256(pd.util.hash_pandas_object(df_sorted, index=True).values).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f405974-8e30-4c32-8ca7-ad6e76997056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_series(series):\n",
    "    series_str = \",\".join(map(str, series.values))\n",
    "    return hashlib.sha256(series_str.encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94f3b365-0a56-4ec0-8aef-6ab3e51f143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_vocab(vocab):\n",
    "    items = sorted(vocab.items(), key=lambda x: x[1])  \n",
    "    vocab_str = \"\\n\".join(f\"{k}:{v}\" for k, v in items)\n",
    "    return hashlib.sha256(vocab_str.encode(\"utf-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb88cf2f-8ee2-4d46-ad41-e0b436925022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_sequences(sequences):\n",
    "    flat = []\n",
    "    for seq in sequences:\n",
    "        flat.extend(seq)\n",
    "        flat.append(-1)  # separator to preserve sequence boundaries\n",
    "    return hashlib.sha256(str(flat).encode(\"utf-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "613f76dc-d2a2-4e59-a5c9-b6c59a74eed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_tensor(tensor):\n",
    "    return hashlib.sha256(tensor.cpu().numpy().tobytes()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc2a1da5-8771-4581-9087-5636b12489c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_number(x):\n",
    "    return hashlib.sha256(str(x).encode(\"utf-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f72b3144-b3cd-4217-be72-8da932f4933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_loss_fn(x):\n",
    "    return hashlib.sha256(type(x).__name__.encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9f5b4a5-2807-4f6f-aadb-2c5389431014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Kid Accidentally Discovered 65 Million Years O...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title category\n",
       "18  Kid Accidentally Discovered 65 Million Years O...  science"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"news.csv\")\n",
    "\n",
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ffd3439-e9ba-4b44-9025-9104e723790c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7c3f6aa068ddf88a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### AUTOMATICALLY GRADED TASK\n",
    "def preprocess_text(df):\n",
    "    \"\"\"\n",
    "    Preprocesses the 'title' column by converting all text to lowercase and removing the news \n",
    "    containing more than 50 words and ensure that the resulting DataFrame has a sequential index. \n",
    "\n",
    "    Returns: \n",
    "        pandas DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    df[\"title\"] = df[\"title\"].str.lower()\n",
    "    df = df[df[\"title\"].str.split().apply(len) <= 50]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98eee94f-f347-4bca-8115-18591f00a2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5223abb-3624-4d61-a814-39ade0cd5dfb",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0baab2b4df7c8f03",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST\n",
    "_df = pd.read_csv(\"news.csv\")\n",
    "_df = preprocess_text(_df)\n",
    "assert hash_data_frame(_df) == \"e703c2a4195678751ef2654c16f87274f58e93445732b3d0763bc53c7d1d6096\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66c19e82-6b03-421a-930f-b873af912d06",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f892bca1e91805c6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### AUTOMATICALLY GRADED TASK\n",
    "def partition_dataset(df):\n",
    "    \"\"\"\n",
    "    Split the dataset into features (X) and target (y), where the feature used is `title` and the target is `category`.\n",
    "    First divide training and testing datasets with 80:20 ratio. Afterwards, the validation dataset is obtaing from the training \n",
    "    dataset using an 80:20 ratio.\n",
    "\n",
    "    If you think encoding is necessary use df[column].astype(\"category\").cat.codes\n",
    "\n",
    "    Use `random_state=42` to ensure reproducibility.\n",
    "    \n",
    "    Return:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame] in the following order: \n",
    "        train_X, val_X, test_X, train_y, val_y, test_y\n",
    "    \"\"\"\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    train_X, test_X, train_y, test_y = train_test_split(df[\"title\"], df[\"category\"].astype(\"category\").cat.codes, test_size=0.2, random_state=42)\n",
    "    train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42)\n",
    "    return train_X, val_X, test_X, train_y, val_y, test_y\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61b0ccf8-190c-459e-b7f2-b0f1e74a6ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, test_X, train_y, val_y, test_y = partition_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69ea2ac0-9f5d-442f-b4d0-a700b7d57e1b",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8ffed2c22dddddda",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## TEST\n",
    "_df = pd.read_csv(\"news.csv\")\n",
    "_train_X, _val_X, _test_X, _train_y, _val_y, _test_y = partition_dataset(_df)\n",
    "assert hash_series(_train_X) == \"59c8ed9a0b2917930e3850c056c94d2e44b8a52b4836e366347ac7c9a6ca43a6\"\n",
    "assert hash_series(_val_X) == \"279ab9652b14dece44420a1cb618558792cfbfaad3b8fe9958f69e8aafaf00f3\"\n",
    "assert hash_series(_test_X) == \"7dbf8caf0a6fbb7adefede4a30a7e8aa286b1a6254985ac135b537712f6e5108\"\n",
    "assert hash_series(_train_y) == \"74f85d07212343107aab5170dfd629a1419839c36b839fad04c44447d255526b\"\n",
    "assert hash_series(_val_y) == \"f268b980b61a2ad78dd31de78dd6cbb37864a2d5ed226e9b4144e6b8ce77dd9b\"\n",
    "assert hash_series(_test_y) == \"933511ffa45cbf19573eba1854f0ac013d638150046f9ce21288657062c4fc47\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24382e94-7298-46e9-8a72-1d279d86bcd8",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-28112ecbca7f291f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### AUTOMATICALLY GRADED TASK\n",
    "def build_vocab(texts):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary dictionary from a list of text strings.\n",
    "\n",
    "    The function should count word frequencies across all input texts and\n",
    "    assign an integer index to each word. The vocabulary must include the\n",
    "    special tokens \"<PAD>\" with index 0 and \"<OOV>\" with index 1. The remaining\n",
    "    words should be added in descending order of frequency, such that all words\n",
    "    from the texts are included.\n",
    "\n",
    "    Parameters:\n",
    "        texts (list of str): List of preprocessed text strings\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping words to integer indices\n",
    "    \"\"\"\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    counter = Counter()\n",
    "\n",
    "    for sentence in texts:\n",
    "        counter.update(sentence.split())\n",
    "\n",
    "    # Special tokens\n",
    "    vocab = {\n",
    "        \"<PAD>\": 0,\n",
    "        \"<OOV>\": 1\n",
    "    }\n",
    "\n",
    "    for idx, (word, _) in enumerate(\n",
    "        counter.most_common(), start=2\n",
    "    ):\n",
    "        vocab[word] = idx\n",
    "\n",
    "    return vocab\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91683856-6659-47d1-96a5-058cc3db81ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(train_X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94ecebca-1df2-48d9-b4fd-0fba3fd6a6e6",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-97dddfea811585b6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## TEST\n",
    "assert hash_vocab(vocab) == \"862a1637b460ef2f0844d5950138b65d518eedc79dced1efa06106fc6e2d4576\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05aea35d-046c-41a9-86d5-d506faf9703c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9564e75951f8cd44",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### AUTOMATICALLY GRADED TASK\n",
    "def texts_to_sequences(texts, vocab):\n",
    "    \"\"\"\n",
    "    Converts a list of text strings into sequences of integer indices using a\n",
    "    given vocabulary.\n",
    "\n",
    "    Each word in a text should be replaced by its corresponding index from the\n",
    "    vocabulary. Words that are not present in the vocabulary must be mapped to\n",
    "    the \"<OOV>\" token.\n",
    "\n",
    "    Parameters:\n",
    "    texts (List[str]): List of preprocessed text strings\n",
    "    vocab (dict): Vocabulary mapping words to integer indices\n",
    "\n",
    "    Returns:\n",
    "        List[List[int]]: List of integer sequences corresponding to the input texts\n",
    "    \"\"\"  \n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    sequences = []\n",
    "\n",
    "    for sentence in texts:\n",
    "        seq = [\n",
    "            vocab.get(word, vocab[\"<OOV>\"])\n",
    "            for word in sentence.split()\n",
    "        ]\n",
    "        sequences.append(seq)\n",
    "\n",
    "    return sequences\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b3228df-2124-492a-80e6-d0336c9bd275",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = texts_to_sequences(train_X.values, vocab)\n",
    "val_X   = texts_to_sequences(val_X.values, vocab)\n",
    "test_X  = texts_to_sequences(test_X.values, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce6d68e5-5b89-4a74-87b1-fc34188d0738",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b7ec99f5c7bd0083",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##TEST\n",
    "assert hash_sequences(train_X) == \"294f732c5d879b074ac644137de52ad86e251f6878350c4784f5e91e0a517905\"\n",
    "assert hash_sequences(val_X) == \"335c1de506a96b5c0504aca00af07c4c8963019d1d7d4a79c6ca454f3370e018\"\n",
    "assert hash_sequences(test_X) == \"24dc4762638be599bd79a4b8fb6b801cebdcb60d8a368b404b8ab541e67cb46a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c9efafe-f28f-4c64-9201-a899f1f99a9d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a303b828d60af25d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### AUTOMATICALLY GRADED TASK\n",
    "def pad(sequences, pad_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of integer sequences so that all sequences have the same length.\n",
    "\n",
    "    Shorter sequences should be padded with the specified padding value until\n",
    "    they match the length of the longest sequence. The output should be a\n",
    "    tensor suitable for batch processing in a neural network.\n",
    "\n",
    "    Parameters:\n",
    "        sequences (List[List[int]]): List of integer sequences\n",
    "        pad_value (int): Value used for padding (default is 0)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Padded tensor of shape (batch_size, max_sequence_length)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    return pad_sequence(\n",
    "        [torch.tensor(seq, dtype=torch.long) for seq in sequences],\n",
    "        batch_first=True,\n",
    "        padding_value=pad_value\n",
    "    )\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed33ff28-fe07-45bc-9b40-95a9c3555c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pad(train_X, pad_value=vocab[\"<PAD>\"])\n",
    "val_X   = pad(val_X,pad_value=vocab[\"<PAD>\"])\n",
    "test_X  = pad(test_X, pad_value=vocab[\"<PAD>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5d88e71-f563-4048-b318-bc739b63d1f7",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-acb1e3cffb7094ef",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##TEST \n",
    "assert hash_tensor(train_X) == \"d0745d7e324a9a9615ead3813de202dcbe8364093c6f0049cc1a0eafc9842461\"\n",
    "assert hash_tensor(val_X) == \"40a61fb08e769b0ce228415d69b846826f4faf92f758dfff8710305c47662139\"\n",
    "assert hash_tensor(test_X) == \"1f6cd72379d003c512c5ec690bae4f119d3c5f201c8f157650e531e0d2d84f7c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcdc4c8e-3404-4eb8-8843-f90da3ef3ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = torch.tensor(train_y.values, dtype=torch.long)\n",
    "val_y   = torch.tensor(val_y.values, dtype=torch.long)\n",
    "test_y  = torch.tensor(test_y.values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b35c95d-24c4-4012-bf3d-02c1b68187a6",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-71791ab858af2537",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_datasets(train_X, train_y, val_X, val_y, test_X, test_y):\n",
    "    \"\"\"\n",
    "    Creates PyTorch TensorDataset objects for the training, validation,\n",
    "    and test splits.\n",
    "\n",
    "    Parameters:\n",
    "        train_X (torch.Tensor): Training input features \n",
    "        train_y (torch.Tensor): Training target labels \n",
    "        val_X (torch.Tensor): Validation input features \n",
    "        val_y (torch.Tensor): Validation target labels \n",
    "        test_X (torch.Tensor): Test input features \n",
    "        test_y (torch.Tensor): Test target labels \n",
    "        \n",
    "    Returns:\n",
    "        train_dataset (TensorDataset): Dataset containing training inputs and targets.\n",
    "        val_dataset (TensorDataset): Dataset containing validation inputs and targets.\n",
    "        test_dataset (TensorDataset): Dataset containing test inputs and targets.\n",
    "    \"\"\"\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    train_dataset = TensorDataset(train_X, train_y)\n",
    "    val_dataset   = TensorDataset(val_X, val_y)\n",
    "    test_dataset  = TensorDataset(test_X, test_y)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ca58458-7a3b-47ce-9ec6-7d4da3a46daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = create_datasets(train_X, train_y, val_X, val_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f68f6f10-e80f-4f59-9a73-e0690e1d5721",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-26138cf90f5d242e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##TEST\n",
    "assert isinstance(train_dataset, TensorDataset)\n",
    "assert isinstance(val_dataset, TensorDataset)\n",
    "assert isinstance(test_dataset, TensorDataset)\n",
    "assert len(train_dataset) == train_X.shape[0] == train_y.shape[0]\n",
    "assert len(val_dataset) == val_X.shape[0] == val_y.shape[0]\n",
    "assert len(test_dataset) == test_X.shape[0] == test_y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7b415c9-8648-4279-824e-6da811917527",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ce1a7fa8022543de",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### AUTOMATICALLY GRADED TASK\n",
    "def create_dataloaders(train_dataset, val_dataset, test_dataset):\n",
    "    \"\"\"\n",
    "    Constructs PyTorch DataLoader objects for the training, validation,\n",
    "    and test datasets with batch size 16.\n",
    "\n",
    "    Parameters:\n",
    "        train_dataset (TensorDataset): Dataset containing training samples.\n",
    "        val_dataset (TensorDataset): Dataset containing validation samples.\n",
    "        test_dataset (TensorDataset): Dataset containing test samples.\n",
    "\n",
    "    Returns:\n",
    "        train_loader (DataLoader): DataLoader that yields mini-batches from the training dataset.\n",
    "        val_loader (DataLoader): DataLoader that yields mini-batches from the validation dataset.\n",
    "        test_loader (DataLoader): DataLoader that yields mini-batches from the test dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7c17c9c-4817-4430-8fc2-11881c6d975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = create_dataloaders(train_dataset, val_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d395d64-eb2d-4eed-a09b-1a33adeaf25a",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-848152dd6e9173d0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##TEST\n",
    "assert isinstance(train_loader, DataLoader)\n",
    "assert isinstance(val_loader, DataLoader)\n",
    "assert isinstance(test_loader, DataLoader)\n",
    "xb, yb = next(iter(train_loader))\n",
    "assert xb.shape[0] <= 32   \n",
    "assert yb.shape[0] == xb.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e26ce8d2-098e-413f-b6a1-06e3d365676a",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-256070cd83d92489",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### AUTOGRADED TASK\n",
    "class TextLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based model for text classification.\n",
    "\n",
    "    The architecture consists of the following components:\n",
    "    1. embedding: An embedding layer that maps token indices to 128-dimensional vectors (use padding_idx=0).\n",
    "    2. dropout1: A dropout layer with a rate of 30% for regularization.\n",
    "    3. lstm: An LSTM layer with 64 hidden units to model sequential dependencies in the input text.\n",
    "    4. dropout2: A dropout layer with a rate of 30% for regularization.\n",
    "    5. fc: A final fully connected linear layer that produces the output logits.\n",
    "\n",
    "    Note:\n",
    "    No activation function is applied in the forward pass. The appropriate\n",
    "    activation is applied implicitly by the loss function during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, 128, padding_idx=0)\n",
    "        self.dropout1 = nn.Dropout(0.30)\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=64, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.30)\n",
    "        self.fc = nn.Linear(64, 3) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)      \n",
    "        x = self.dropout1(x)                \n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.dropout2(x)                \n",
    "        return self.fc(x).squeeze(1)                \n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ac20726-c4f6-43b5-b24f-fe367f61ee8d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b81c8223a7699826",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### AUTOGRADED TASK\n",
    "def initialize_model():\n",
    "    \"\"\"\n",
    "    Initializes and returns a TextLSTM model.\n",
    "\n",
    "    Parameters:\n",
    "        vocab_size (int): Size of the vocabulary\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module\n",
    "    \"\"\"\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    return TextLSTM(len(vocab))\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1432915-25c6-4912-8cc5-d327b231eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d3ac770-5636-47e1-9933-8e523d67a480",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1125c13fbf0a8b6d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##TEST\n",
    "assert hasattr(model, \"embedding\")\n",
    "assert model.embedding.embedding_dim == 128\n",
    "assert hash_number(model.embedding.num_embeddings) == \"782c60cb3f190b381e1bed1b8ee205b00d59fc1241c28ed88b169d092be94892\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f3bc8b6-cfa2-4d85-bd99-4ef3f89261ef",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2024b055782dbab5",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##TEST\n",
    "assert hasattr(model, \"dropout1\") \n",
    "assert model.dropout1.p == 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e822e36-22e3-4d8b-a18d-33136313c45b",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-553572c839d22564",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##TEST\n",
    "assert hasattr(model, \"lstm\")\n",
    "assert model.lstm.input_size == 128\n",
    "assert model.lstm.hidden_size == 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d38fd9da-126b-4946-ae92-15b0e2c09831",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-06866131cde3e4fb",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "###TEST\n",
    "assert hasattr(model, \"dropout2\") \n",
    "assert model.dropout2.p == 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "13e01be4-5c6d-4528-8418-5242ec98eb28",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-63a9ebfe9d0bea0f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##TEST\n",
    "assert hasattr(model, \"fc\") \n",
    "assert model.fc.in_features == 64\n",
    "assert hash_number(model.fc.out_features) == \"4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e66af194-2b40-4612-8e13-aefb0e5774d1",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5b2d76c3fa42d07a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### AUTOGRADED TASK\n",
    "def get_loss_fn():\n",
    "    \"\"\"\n",
    "    Define the loss function.\n",
    "\n",
    "    Hint:\n",
    "    Since no activation function is defined at the output layer, you should choose\n",
    "    a loss function suitable for this task that internally applies the appropriate\n",
    "    activation.\n",
    "\n",
    "    Returns:\n",
    "        loss_fn\n",
    "    \"\"\"\n",
    "        \n",
    "    ### BEGIN SOLUTION\n",
    "    return nn.CrossEntropyLoss()\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0b873b12-f6f6-4448-ad07-5b63b3561c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = get_loss_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f775091e-dfc0-4836-b827-ed7ccf708608",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9676eb2325a9200e",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##TEST\n",
    "assert hash_loss_fn(loss_fn) == \"26e028292be7fff8f121074eb70733520c493600aff2bf5989b7c13a840ce9d2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "100bc449-26cb-4be3-b614-be6eeee01012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for features, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(features)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "915e0c33-5620-448b-8be5-f0a9f788efdf",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e0da6a655b78d9b2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### AUTOGRADED TASK\n",
    "def evaluate(model, dataloader, loss_fn):\n",
    "    \"\"\"\n",
    "    Transform the logits in order to get the class prediction.\n",
    "    \"\"\"\n",
    " \n",
    "    model.eval()\n",
    "\n",
    "    total_loss, correct, total = (0,)*3\n",
    "    with torch.no_grad():\n",
    "        for features, targets in dataloader:\n",
    "            outputs = model(features)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            ### BEGIN SOLUTION\n",
    "            predictions = torch.argmax(outputs, dim=1)           \n",
    "            ### END SOLUTION\n",
    "\n",
    "            correct += (predictions == targets).sum().item()\n",
    "            total += targets.numel()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return total_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5bab814d-f4e7-4b5a-97fc-4bcf30aa2bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, val_loader, loss_fn, optimizer, num_epochs):\n",
    "    train_losses, val_losses = [], []\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = train(model, train_loader, loss_fn, optimizer)\n",
    "        train_losses.append(train_loss)\n",
    "        val_loss, _ = evaluate(model, val_loader, loss_fn)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch:3d}/{num_epochs} | train_loss={train_loss:.4f} val_loss={val_loss:.4f}\")\n",
    "                  \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb80621-2105-4355-94fc-7e0e7715e63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747d830e-4826-4c49-b200-e290a4d3af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses = train_and_evaluate(model, train_loader, val_loader, loss_fn, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd184cd2-382c-421a-819f-27f0ca8bf05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=range(1, num_epochs + 1), y=train_losses, label=\"train_loss\")\n",
    "sns.lineplot(x=range(1, num_epochs + 1), y=val_losses, label=\"val_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb0033b-74db-450f-8b49-c93e9b4222ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_loader, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2595ec4-8422-401d-9c17-559ee06a79cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b5f3a-8964-44e6-b0b6-e2e1432286a9",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-314a506ce536fdfd",
     "locked": true,
     "points": 8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##TEST\n",
    "assert isinstance(test_acc, float)\n",
    "assert 0.4 <= test_acc <= 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (i2ds-venv)",
   "language": "python",
   "name": "i2ds-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
